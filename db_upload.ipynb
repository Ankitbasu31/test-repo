{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd \n",
    "import pyodbc\n",
    "import nest_asyncio\n",
    "import os \n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "\n",
    "nest_asyncio.apply()\n",
    "pd.options.mode.chained_assignment = None # suppressed warning if writing to df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today()\n",
    "current_year = today.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar, datetime as dt\n",
    "\n",
    "def get_date(day, month, year):\n",
    "    \"\"\"\n",
    "    Takes three integer values: day, month and year, and converts them to a datetime object\n",
    "    \"\"\"\n",
    "    # handle two digit years \n",
    "    if year < 100:\n",
    "        year += 1900 if year >= 50 else 2000\n",
    "        \n",
    "    calendar_days = calendar.monthrange(year, month)[1]\n",
    "    # check if day is less than 1 and adjust to previous month\n",
    "    if day < 1:\n",
    "        # shift back month by 1\n",
    "        \n",
    "        month -= 1 \n",
    "        if month < 1:\n",
    "            # adjust the month \n",
    "            month = 12 \n",
    "            year -= 1\n",
    "        \n",
    "        calendar_days = calendar.monthrange(year, month)[1]\n",
    "        day = calendar_days\n",
    "\n",
    "    if day > calendar_days:\n",
    "        date_obj = datetime(year, month, 1) + dt.timedelta(day - 1)\n",
    "    else:\n",
    "        date_obj = datetime(year, month, day)\n",
    "\n",
    "      \n",
    "    return date_obj.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of frequently used columns\n",
    "cols = ['AGE',\n",
    " 'BLDSTOOL',\n",
    " 'CHECKUP',\n",
    " 'COLNCNCR',\n",
    " 'COLNSCPY',\n",
    " 'COLNTEST',\n",
    " 'FINALWT',\n",
    " 'HADMAM',\n",
    " 'HADSGCOL',\n",
    " 'HADSIGM',\n",
    " 'HIVRISK',\n",
    " 'HIVTEST',\n",
    " 'HLTHPLAN',\n",
    " 'HOWLONG',\n",
    " 'IDAY',\n",
    " 'IMONTH',\n",
    " 'INCOME2',\n",
    " 'IYEAR',\n",
    " 'LASTSIGM',\n",
    " 'LSTBLDST',\n",
    " 'PERSDOC',\n",
    " 'SDNATEST',\n",
    " 'SEX',\n",
    " 'SIGMSCPY',\n",
    " 'STOOLDNA',\n",
    " 'VCLTEST',\n",
    " 'VIRCOLON',\n",
    " '_AGEG5YR']\n",
    "\n",
    "# column maps \n",
    "colmap = {\n",
    "    'age': 'AGE',\n",
    " '_ageg5yr': '_AGEG5YR',\n",
    " 'persdoc': 'PERSDOC',\n",
    " 'checkup': 'CHECKUP',\n",
    " 'sex': 'SEX',\n",
    "  'hlthplan': 'HLTHPLAN',\n",
    " 'hivtest': 'HIVTEST',\n",
    " 'hivrisk': 'HIVRISK',\n",
    " 'wcol': 'FINALWT',\n",
    " 'income2': 'INCOME2',\n",
    " 'howlong': 'HOWLONG',\n",
    " 'hadmam': 'HADMAM',\n",
    " 'hadsigm': 'HADSIGM',\n",
    " 'hadsgcol': 'HADSGCOL',\n",
    " 'bldstool': 'BLDSTOOL',\n",
    " 'lastsigm': 'LASTSIGM',\n",
    " 'lstbldst': 'LSTBLDST',\n",
    " 'colntest': 'COLNTEST',\n",
    " 'stooldna': 'STOOLDNA',\n",
    " 'vclntest': 'VCLNTEST',\n",
    " 'sdnatest': 'SDNATEST',\n",
    " 'vircolon': 'VIRCOLON',\n",
    " 'colnscpy': 'COLNSCPY',\n",
    " 'sigmscpy': 'SIGMSCPY',\n",
    " 'colncncr': 'COLNCNCR',\n",
    " 'iyear': 'IYEAR',\n",
    " 'imonth': 'IMONTH',\n",
    " 'iday': 'IDAY'\n",
    "}\n",
    "\n",
    "# update to start year at the most recent year of update to prevent attempting to build this dataset from scratch\n",
    "start_year = 2022\n",
    "year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the BRFSS data breakdown file which contains the updated column names\n",
    "df = pd.read_excel(r\"C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\Input folder - zip\\BRFSS_data_breakdown.xlsx\", \"vars\", index_col='year')\n",
    "# dictionary to host DataFrames \n",
    "frames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataset already exists \n",
    "# accumulated_path = r'S:\\LON_SSRES\\DataAnalytics\\US Life Science\\Data\\Input\\BRFSS_COLORECTAL.csv'\n",
    "accumulated_path = r'C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\Input folder - zip\\BRFSS Filtered Dataset - Copy.csv'\n",
    "\n",
    "if os.path.exists(accumulated_path):\n",
    "    # if dataset exists, then we want to import the data so that it can be updated\n",
    "    df2 = pd.read_csv(accumulated_path, parse_dates=['IDATE'])\n",
    "    df2.rename(columns = {'VCLTEST': 'VCLNTEST'}, inplace=True)\n",
    "else:\n",
    "    # if the dataset doesn't exist, create an empty dataframe. Reduces chance of error\n",
    "    df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate df2 then append the new dat to the frame\n",
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 2022 data: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'R:\\\\Life science tools and diagnostics\\\\Topical analyses\\\\EXAS deep dive\\\\BRFSS\\\\Jasmeen\\\\Data\\\\LLCP2022XPT\\\\LLCP2022.XPT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m data\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m year)\n\u001b[1;32m---> 11\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m row \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     13\u001b[0m indx \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfileloc\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sas\\sasreader.py:154\u001b[0m, in \u001b[0;36mread_sas\u001b[1;34m(filepath_or_buffer, format, index, encoding, chunksize, iterator, compression)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxport\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas_xport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XportReader\n\u001b[1;32m--> 154\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43mXportReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msas7bdat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msas7bdat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAS7BDATReader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sas\\sas_xport.py:270\u001b[0m, in \u001b[0;36mXportReader.__init__\u001b[1;34m(self, filepath_or_buffer, index, encoding, chunksize, compression)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunksize \u001b[38;5;241m=\u001b[39m chunksize\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'R:\\\\Life science tools and diagnostics\\\\Topical analyses\\\\EXAS deep dive\\\\BRFSS\\\\Jasmeen\\\\Data\\\\LLCP2022XPT\\\\LLCP2022.XPT'"
     ]
    }
   ],
   "source": [
    "# Create progress bar to track progres of the data reading\n",
    "bar = tqdm(df.loc[start_year:, :].iterrows(), desc=('Reading %s row:' % year))\n",
    "# bar = tqdm(df.iterrows(), desc=('Reading %s row:' % year))\n",
    "\n",
    "for year, row in bar:\n",
    "    # identify file location and create output path name in case of code break \n",
    "    folder, filename = os.path.split(row.fileloc)\n",
    "    output_filename = os.path.splitext(filename)[0] +'.csv'\n",
    "\n",
    "    bar.set_description('Reading %s data' % year)\n",
    "    df1 = pd.read_sas(row.fileloc)\n",
    "    row = row.dropna()\n",
    "    indx = [x for x in row.index if not x in ['fileloc']]\n",
    " \n",
    "    \n",
    "    bar.set_description(f'Updating {year} column names...')\n",
    "    df1 = df1.rename(columns={v.upper():colmap[k] for k,v in row.loc[indx].items()})\n",
    "    df1['IDATE'] = df1.apply(lambda row: get_date(day=int(row.IDAY), month=int(row.IMONTH), year=int(row.IYEAR)),axis=1)\n",
    "    df1['DATAYEAR'] = year\n",
    "\n",
    "    bar.set_description(f'Appending {year} to full dataset ...')\n",
    "    data = pd.concat([data, df1], ignore_index=True)\n",
    "\n",
    "    bar.set_description(f'Saving {year} as {filename}.csv ...')\n",
    "    df1.to_csv(os.path.join(folder, output_filename), index=False)\n",
    "\n",
    "\n",
    "    bar.set_description(f'Adding {year} to frames dictionary ...')\n",
    "    frames[year] = df1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtatan\\AppData\\Local\\Temp\\ipykernel_26204\\3616385677.py:4: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(os.path.join(folder,output_filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating CDBRFS98.csv ...\n",
      "Updating CDBRFS99.csv ...\n",
      "Updating CDBRFS00.csv ...\n",
      "Updating CDBRFS01.csv ...\n",
      "Updating cdbrfs02.csv ...\n",
      "Updating cdbrfs03.csv ...\n",
      "Updating CDBRFS04.csv ...\n",
      "Updating CDBRFS05.csv ...\n",
      "Updating CDBRFS06.csv ...\n",
      "Updating CDBRFS07.csv ...\n",
      "Updating CDBRFS08.csv ...\n",
      "Updating CDBRFS09.csv ...\n",
      "Updating CDBRFS10.csv ...\n"
     ]
    }
   ],
   "source": [
    "# for year, row in df.loc[1998:2010].iterrows():\n",
    "#     folder, filename = os.path.split(row.fileloc)\n",
    "#     output_filename = os.path.splitext(filename)[0] +'.csv'\n",
    "#     df1 = pd.read_csv(os.path.join(folder,output_filename))\n",
    "#     types = df1.dtypes\n",
    "#     notfloats = types[~types.eq(float)]\n",
    "#     for col in notfloats.keys():\n",
    "#         try:\n",
    "#             df1[col] = df1[col].apply(lambda x: eval(x).decode('utf-8'))\n",
    "#         except:\n",
    "#             continue\n",
    "#         if col == 'INTVID':\n",
    "#             continue\n",
    "#         elif col == 'IDATE':\n",
    "#             df1[col] = df1[col].apply(get_date)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 df1[col] = df1[col].apply(lambda x: float(x) if len(x) > 0 else None)\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "#     # update float types again:\n",
    "#     types = df1.dtypes\n",
    "#     floats_only = types[types.eq(float)]\n",
    "#     for col in floats_only.keys():\n",
    "#         df1[col] = df1[col].apply(lambda x: x if not x.is_integer() else int(x))\n",
    "        \n",
    "#     # rename columns\n",
    "#     row = row.dropna()\n",
    "#     indx = [x for x in row.index if not x in ['fileloc']]\n",
    "    \n",
    "#     df1 = df1.rename(columns={v:colmap[k] for k,v in row.loc[indx].items()})\n",
    "#     print('Updating', output_filename, '...')\n",
    "#     df1.to_csv(os.path.join(folder,output_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading LLCP2023.csv...: : 3it [03:40, 73.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm \n",
    "# bar = tqdm(df.loc[start_year:].iterrows())\n",
    "# # bar = tqdm(frames.items())\n",
    "# # this will hold the data from the newly added data\n",
    "# data = pd.DataFrame()\n",
    "\n",
    "# for year, row in bar:\n",
    "#     # get the file location\n",
    "#     folder, filename = os.path.split(row.fileloc)\n",
    "#     # define output file hname\n",
    "#     output_filename = os.path.splitext(filename)[0] +'.csv'\n",
    "    \n",
    "\n",
    "#     bar.set_description(f'Reading {output_filename}...')\n",
    "    \n",
    "#     if not year in frames:\n",
    "#         # read the CSV file that was created earlier to \n",
    "#         df1 = pd.read_csv(os.path.join(folder,output_filename), usecols=lambda x: x in cols,dtype={'IMONTH': bytes, 'IDAY': bytes, 'IYEAR':bytes})\n",
    "#         # the day, month and year variables might be stored as bytes. This will transform them into ints \n",
    "#         df1['IDAY'] = df1['IDAY'].apply(eval).astype(int)\n",
    "#         df1['IMONTH'] = df1['IMONTH'].apply(eval).astype(int)\n",
    "#         df1['IYEAR'] = df1['IYEAR'].apply(eval).astype(int)\n",
    "\n",
    "#     else:\n",
    "#         df1 = frames[year]\n",
    "\n",
    "#     df1['IDATE'] = df1.apply(lambda row: get_date(day=int(row.IDAY), month=int(row.IMONTH), year=int(row.IYEAR)),axis=1)\n",
    "#     df1['DATAYEAR'] = year\n",
    "#     data = pd.concat([data, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.concat([df2, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv(r'S:\\LON_SSRES\\DataAnalytics\\US Life Science\\Data\\Input\\2024\\BRFSS_COLORECTAL.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {'int64': 'numeric(18,0)', 'float64': 'decimal(28,0)', 'object': 'varchar(50)', 'datetime64[ns]': 'datetime' }\n",
    "dtypes = data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'int32'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m] \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtypes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[170], line 1\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes[v\u001b[38;5;241m.\u001b[39mname]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mitems()))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'int32'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print(',\\n'.join(f'[{k}] {types[v.name]}' for k, v in dtypes.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
